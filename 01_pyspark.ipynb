{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e51325c",
   "metadata": {},
   "source": [
    "> **Question 1. **\n",
    ">\n",
    "![alt text](question_ss\\01.jpg \"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9cac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, col, coalesce, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a8fe34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"seperate_hobbies\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63a94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('Alice', 'Badminton, Tennis'), ('Bob', 'Tennis, Cricket'), ('Julis', 'Cricket, Carrom')]\n",
    "columns = [\"Name\", \"Hobbies\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f76100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "| Name|          Hobbies|\n",
      "+-----+-----------------+\n",
      "|Alice|Badminton, Tennis|\n",
      "|  Bob|  Tennis, Cricket|\n",
      "|Julis|  Cricket, Carrom|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a949ef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_PYTHON: C:\\Users\\Prasad\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "PYSPARK_DRIVER_PYTHON: C:\\Users\\Prasad\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"PYSPARK_PYTHON:\", os.environ.get(\"PYSPARK_PYTHON\"))\n",
    "print(\"PYSPARK_DRIVER_PYTHON:\", os.environ.get(\"PYSPARK_DRIVER_PYTHON\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae536dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "| Name|             Hobbies|\n",
      "+-----+--------------------+\n",
      "|Alice|[Badminton,  Tennis]|\n",
      "|  Bob|  [Tennis,  Cricket]|\n",
      "|Julis|  [Cricket,  Carrom]|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split by comma which convert Hobbies from str -> Array \n",
    "df.select( col(\"Name\"), split(col(\"Hobbies\"), \",\").alias(\"Hobbies\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bcee15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| Name|  Hobbies|\n",
      "+-----+---------+\n",
      "|Alice|Badminton|\n",
      "|Alice|   Tennis|\n",
      "|  Bob|   Tennis|\n",
      "|  Bob|  Cricket|\n",
      "|Julis|  Cricket|\n",
      "|Julis|   Carrom|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode function convert array to multiple rows\n",
    "df.select( col(\"Name\"), explode(split(col(\"Hobbies\"), \",\")).alias(\"Hobbies\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb97ba",
   "metadata": {},
   "source": [
    "![alt text](question_ss\\02.jpg \"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b90c8f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|city1| city2| city3|\n",
      "+-----+------+------+\n",
      "|  Goa|      |Mumbai|\n",
      "|     |Mumbai|  NULL|\n",
      "| NULL|      |  Pune|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Goa', '', 'Mumbai'), ('', 'Mumbai', None), (None, '', 'Pune')]\n",
    "columns = ['city1', 'city2', 'city3']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5249e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|firstnotnull|\n",
      "+------------+\n",
      "|         Goa|\n",
      "|      Mumbai|\n",
      "|        Pune|\n",
      "+------------+\n",
      "\n",
      "+-----+------+------+\n",
      "|city1| city2| city3|\n",
      "+-----+------+------+\n",
      "|  Goa|      |Mumbai|\n",
      "|     |Mumbai|  NULL|\n",
      "| NULL|      |  Pune|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This creates a new column firstnotnull that contains the first non-empty value among city1, city2, and city3 for each row.\n",
    "# coalesce() Returns the first column that is not null.\n",
    "df1 = df.withColumn(\n",
    "    'firstnotnull', \n",
    "    coalesce(\n",
    "        when(df['city1']=='', None).otherwise(df['city1']), \n",
    "        when(df['city2']=='', None).otherwise(df['city2']),\n",
    "        when(df['city3']=='', None).otherwise(df['city3'])\n",
    "    )\n",
    ")\n",
    "df1.select('firstnotnull').show()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829748d",
   "metadata": {},
   "source": [
    "> **3. Claculate the % Marks for each student. Each student subject is of 100 marks. Create a result by following the below condition.**\n",
    ">\n",
    "![alt text](question_ss\\03.jpg \"question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb6427",
   "metadata": {},
   "source": [
    "> Answer\n",
    ">\n",
    "![alt text](question_ss\\03a.jpg \"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0934e946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| Id| Name|\n",
      "+---+-----+\n",
      "|  1|Steve|\n",
      "|  2|David|\n",
      "|  3| John|\n",
      "|  4|Shree|\n",
      "|  5|Helen|\n",
      "+---+-----+\n",
      "\n",
      "+---+-------+----+\n",
      "| Id|Subject|Mark|\n",
      "+---+-------+----+\n",
      "|  1|    SQL|  40|\n",
      "|  1|PySpark| 100|\n",
      "|  2|    SQL|  70|\n",
      "|  2|PySpark|  60|\n",
      "|  3|    SQL|  30|\n",
      "|  3|PySpark|  20|\n",
      "|  4|    SQL|  50|\n",
      "|  4|PySpark|  50|\n",
      "|  5|    SQL|  45|\n",
      "|  5|PySpark|  45|\n",
      "+---+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1, \"Steve\"), (2, 'David'), (3, 'John'), (4, 'Shree'), (5, 'Helen')]\n",
    "data2 = [(1, 'SQL', 40), (1, 'PySpark', 100), (2, 'SQL', 70), (2, 'PySpark', 60), (3, 'SQL', 30), (3, 'PySpark', 20), (4, 'SQL', 50), (4, 'PySpark', 50), (5, 'SQL', 45), (5, 'PySpark', 45)]\n",
    "\n",
    "schema1 = ['Id', 'Name']\n",
    "schema2 = ['Id', 'Subject', 'Mark']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5d655f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+----+\n",
      "| Id| Name|Subject|Mark|\n",
      "+---+-----+-------+----+\n",
      "|  1|Steve|    SQL|  40|\n",
      "|  1|Steve|PySpark| 100|\n",
      "|  2|David|    SQL|  70|\n",
      "|  2|David|PySpark|  60|\n",
      "|  3| John|    SQL|  30|\n",
      "|  3| John|PySpark|  20|\n",
      "|  4|Shree|    SQL|  50|\n",
      "|  4|Shree|PySpark|  50|\n",
      "|  5|Helen|    SQL|  45|\n",
      "|  5|Helen|PySpark|  45|\n",
      "+---+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: combine both tables or dataframes by using join\n",
    "df_join = df1.join(df2, df1['Id']==df2['Id']).drop(df2['Id'])\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24634864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01a60171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n",
      "| Id| Name|Percentage|\n",
      "+---+-----+----------+\n",
      "|  1|Steve|      70.0|\n",
      "|  2|David|      65.0|\n",
      "|  3| John|      25.0|\n",
      "|  4|Shree|      50.0|\n",
      "|  5|Helen|      45.0|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Calculating the percentage by groupby() on Id and Name column\n",
    "#  then sum of marks divided by total count\n",
    "df_per = df_join.groupBy('Id', 'Name').agg((sum(col('Mark'))/count('*')).alias('Percentage'))\n",
    "df_per.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45227bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+------------+\n",
      "| Id| Name|Percentage|      Result|\n",
      "+---+-----+----------+------------+\n",
      "|  1|Steve|      70.0| Distinction|\n",
      "|  2|David|      65.0| First Class|\n",
      "|  3| John|      25.0|        Fail|\n",
      "|  4|Shree|      50.0|Second Class|\n",
      "|  5|Helen|      45.0| Third Class|\n",
      "+---+-----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: use when() and otherwise() to get Result\n",
    "result = df_per.select(\n",
    "    '*', \n",
    "    (\n",
    "        when(df_per['Percentage'] >=70, 'Distinction')\n",
    "        .when((df_per['Percentage'] < 70) & (df_per['Percentage'] >= 60), 'First Class')\n",
    "        .when((df_per['Percentage'] < 60) & (df_per['Percentage'] >= 50), 'Second Class')\n",
    "        .when((df_per['Percentage'] < 50) & (df_per['Percentage'] >= 40), 'Third Class')\n",
    "        .when(df_per['Percentage'] < 40, 'Fail')\n",
    "    ).alias('Result')\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951328e7",
   "metadata": {},
   "source": [
    "> **3. Department wise nth highest salary employees.**\n",
    ">\n",
    "![alt text](question_ss\\04.jpg \"question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd49010",
   "metadata": {},
   "source": [
    "> Answer\n",
    ">\n",
    "![alt text](question_ss\\04a.jpg \"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b8ed052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+--------+\n",
      "|EmpId|EmpName|Salary|DeptName|\n",
      "+-----+-------+------+--------+\n",
      "|    1|      A|  1000|      IT|\n",
      "|    2|      B|  1500|      IT|\n",
      "|    3|      C|  2500|      IT|\n",
      "|    4|      D|  3000|      HR|\n",
      "|    5|      E|  2000|      HR|\n",
      "|    6|      F|  1000|      HR|\n",
      "|    7|      G|  4000|   Sales|\n",
      "|    8|      H|  4000|   Sales|\n",
      "|    9|      I|  1000|   Sales|\n",
      "|   10|      J|  2000|   Sales|\n",
      "+-----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1=[(1,\"A\",1000,\"IT\"),(2,\"B\",1500,\"IT\"),(3,\"C\",2500,\"IT\"),(4,\"D\",3000,\"HR\"),(5,\"E\",2000,\"HR\"),(6,\"F\",1000,\"HR\")\n",
    "       ,(7,\"G\",4000,\"Sales\"),(8,\"H\",4000,\"Sales\"),(9,\"I\",1000,\"Sales\"),(10,\"J\",2000,\"Sales\")]\n",
    "schema1=[\"EmpId\",\"EmpName\",\"Salary\",\"DeptName\"]\n",
    "df=spark.createDataFrame(data1,schema1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c847e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d9f3c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+--------+----+\n",
      "|EmpId|EmpName|Salary|DeptName|rank|\n",
      "+-----+-------+------+--------+----+\n",
      "|    4|      D|  3000|      HR|   1|\n",
      "|    5|      E|  2000|      HR|   2|\n",
      "|    6|      F|  1000|      HR|   3|\n",
      "|    3|      C|  2500|      IT|   1|\n",
      "|    2|      B|  1500|      IT|   2|\n",
      "|    1|      A|  1000|      IT|   3|\n",
      "|    7|      G|  4000|   Sales|   1|\n",
      "|    8|      H|  4000|   Sales|   1|\n",
      "|   10|      J|  2000|   Sales|   2|\n",
      "|    9|      I|  1000|   Sales|   3|\n",
      "+-----+-------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rank = df.select(\n",
    "    '*',\n",
    "    dense_rank().over(\n",
    "        Window.partitionBy(df['DeptName'])\n",
    "        .orderBy(df['Salary'].desc())\n",
    "    ).alias('rank')\n",
    ")\n",
    "\n",
    "df_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "459f4e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+--------+----+\n",
      "|EmpId|EmpName|Salary|DeptName|rank|\n",
      "+-----+-------+------+--------+----+\n",
      "|    4|      D|  3000|      HR|   1|\n",
      "|    3|      C|  2500|      IT|   1|\n",
      "|    7|      G|  4000|   Sales|   1|\n",
      "|    8|      H|  4000|   Sales|   1|\n",
      "+-----+-------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = df_rank.filter(df_rank.rank==1)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402e21c",
   "metadata": {},
   "source": [
    "> **5.**\n",
    ">\n",
    "![alt text](question_ss\\05.jpg \"question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea122bb",
   "metadata": {},
   "source": [
    "> Answer\n",
    ">\n",
    "![alt text](question_ss\\05a.jpg \"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68f01619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+------+--------+------+\n",
      "|EmpId|EmpName|Mgrid|deptid|salarydt|salary|\n",
      "+-----+-------+-----+------+--------+------+\n",
      "|  100|    Raj| NULL|     1|01-04-23| 50000|\n",
      "|  200| Joanne|  100|     1|01-04-23|  4000|\n",
      "|  200| Joanne|  100|     1|13-04-23|  4500|\n",
      "|  200| Joanne|  100|     1|14-04-23|  4020|\n",
      "+-----+-------+-----+------+--------+------+\n",
      "\n",
      "+------+--------+\n",
      "|deptid|deptname|\n",
      "+------+--------+\n",
      "|     1|      IT|\n",
      "|     2|      HR|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Employees Salary info\n",
    "data1=[(100,\"Raj\",None,1,\"01-04-23\",50000), (200,\"Joanne\",100,1,\"01-04-23\",4000),\n",
    "       (200,\"Joanne\",100,1,\"13-04-23\",4500),(200,\"Joanne\",100,1,\"14-04-23\",4020)]\n",
    "\n",
    "schema1=[\"EmpId\",\"EmpName\",\"Mgrid\",\"deptid\",\"salarydt\",\"salary\"]\n",
    "\n",
    "df_salary=spark.createDataFrame(data1,schema1)\n",
    "df_salary.show()\n",
    "\n",
    "#department dataframe\n",
    "data2=[(1,\"IT\"), (2,\"HR\")]\n",
    "schema2=[\"deptid\",\"deptname\"]\n",
    "\n",
    "df_dept=spark.createDataFrame(data2,schema2)\n",
    "df_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e67ba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+------+--------+------+----------+\n",
      "|EmpId|EmpName|Mgrid|deptid|salarydt|salary|  Newsaldt|\n",
      "+-----+-------+-----+------+--------+------+----------+\n",
      "|  100|    Raj| NULL|     1|01-04-23| 50000|2023-04-01|\n",
      "|  200| Joanne|  100|     1|01-04-23|  4000|2023-04-01|\n",
      "|  200| Joanne|  100|     1|13-04-23|  4500|2023-04-13|\n",
      "|  200| Joanne|  100|     1|14-04-23|  4020|2023-04-14|\n",
      "+-----+-------+-----+------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reformat date column \n",
    "df = df_salary.withColumn('Newsaldt', to_date(col('salarydt'), 'dd-MM-yy'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "448ec0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+----------+------+\n",
      "|deptname|ManagerName|EmpName|  Newsaldt|salary|\n",
      "+--------+-----------+-------+----------+------+\n",
      "|      IT|       NULL|    Raj|2023-04-01| 50000|\n",
      "|      IT|        Raj| Joanne|2023-04-01|  4000|\n",
      "|      IT|        Raj| Joanne|2023-04-13|  4500|\n",
      "|      IT|        Raj| Joanne|2023-04-14|  4020|\n",
      "+--------+-----------+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join both df, join with only ['deptid] so that it will remove duplicate deptid col\n",
    "df_join = df.join(df_dept, ['deptid'])\n",
    "\n",
    "# Self join to get manager details\n",
    "df_joined = df_join.alias('tbl1').join(\n",
    "\tdf_join.alias('tbl2'),\n",
    "\tcol('tbl1.Mgrid') == col('tbl2.EmpId'),\n",
    "\t'left'\n",
    ").select(\n",
    "    col('tbl1.deptname'),\n",
    "\tcol('tbl2.EmpName').alias('ManagerName'),\n",
    "\tcol('tbl1.EmpName').alias('EmpName'),\n",
    "\tcol('tbl1.Newsaldt').alias('Newsaldt'),\n",
    "\tcol('tbl1.salary').alias('salary'),\n",
    ")\n",
    "\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "349cd715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+----+-----+-----------+\n",
      "|deptname|ManagerName|EmpName|Year|Month|sum(salary)|\n",
      "+--------+-----------+-------+----+-----+-----------+\n",
      "|      IT|        Raj| Joanne|2023|  Apr|      12520|\n",
      "|      IT|       NULL|    Raj|2023|  Apr|      50000|\n",
      "+--------+-----------+-------+----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby\n",
    "res = df_joined.groupBy('deptname', 'ManagerName', 'EmpName', year('Newsaldt').alias('Year'), date_format('Newsaldt', 'MMM').alias('Month')).sum('salary')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec621095",
   "metadata": {},
   "source": [
    "> **6. How to check data skew Issue and how to solve it.**\n",
    ">\n",
    "![alt text](question_ss\\06.jpg \"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79fcde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e00bee99",
   "metadata": {},
   "source": [
    "> **6. Merger two dataframes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd0cc099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------------+----+-----+\n",
      "| ID|Student_Name|Department_Name|City|Marks|\n",
      "+---+------------+---------------+----+-----+\n",
      "|  1|       Sagar|            CSE|  UP|   80|\n",
      "|  2|      Shivam|             IT|  MP|   86|\n",
      "|  3|        Muni|           Mech|  AP|   70|\n",
      "+---+------------+---------------+----+-----+\n",
      "\n",
      "+---+------------+---------------+---------+\n",
      "| ID|Student_Name|Department_Name|     City|\n",
      "+---+------------+---------------+---------+\n",
      "|  5|         Raj|            CSE|       HP|\n",
      "|  7|       Kunal|           Mech|Rajasthan|\n",
      "+---+------------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(1, \"Sagar\", \"CSE\", \"UP\", 80), (2, \"Shivam\", \"IT\", \"MP\", 86), (3, \"Muni\", \"Mech\", \"AP\", 70)]\n",
    "\n",
    "simpleData_2 = [(5, \"Raj\", \"CSE\", \"HP\"), (7, \"Kunal\", \"Mech\", \"Rajasthan\")]\n",
    "\n",
    "\n",
    "columns_1 = [\"ID\", \"Student_Name\", \"Department_Name\", \"City\", \"Marks\"]\n",
    "columns_2 = [\"ID\", \"Student_Name\", \"Department_Name\", \"City\"]\n",
    "\n",
    "df_1 = spark.createDataFrame(data = simpleData, schema = columns_1)\n",
    "df_2 = spark.createDataFrame(data = simpleData_2, schema = columns_2)\n",
    "df_1.show()\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7f0cdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------------+---------+-----+\n",
      "| ID|Student_Name|Department_Name|     City|Marks|\n",
      "+---+------------+---------------+---------+-----+\n",
      "|  1|       Sagar|            CSE|       UP|   80|\n",
      "|  2|      Shivam|             IT|       MP|   86|\n",
      "|  3|        Muni|           Mech|       AP|   70|\n",
      "|  5|         Raj|            CSE|       HP| NULL|\n",
      "|  7|       Kunal|           Mech|Rajasthan| NULL|\n",
      "+---+------------+---------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_1.unionByName(df_2, allowMissingColumns=True)\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
