{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feba0e66-15ea-47b0-9da8-ff5e9ed497de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **PYSPARK INTERVIEW QUESTIONS - PRASAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2e9a3a-8042-41be-97f6-873602d6643b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e79732b-ea84-4edb-8d6a-e944a5955363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q1 While ingesting customer data from an external source, you notice duplicate entries. How would you remove duplicates and retain only the latest entry based on a timestamp column?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762ef2f8-b82b-4154-9c35-edc63545f8b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>date</th><th>sales</th></tr></thead><tbody><tr><td>101</td><td>2023-12-01</td><td>100</td></tr><tr><td>101</td><td>2023-12-02</td><td>150</td></tr><tr><td>102</td><td>2023-12-01</td><td>200</td></tr><tr><td>102</td><td>2023-12-02</td><td>250</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "2023-12-01",
         100
        ],
        [
         "101",
         "2023-12-02",
         150
        ],
        [
         "102",
         "2023-12-01",
         200
        ],
        [
         "102",
         "2023-12-02",
         250
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"101\", \"2023-12-01\", 100), (\"101\", \"2023-12-02\", 150), \n",
    "        (\"102\", \"2023-12-01\", 200), (\"102\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d480f60c-5dcb-4b8d-81de-48964a0c249d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "284e6d78-afac-40e9-9e80-6f8180c144ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>date</th><th>sales</th></tr></thead><tbody><tr><td>101</td><td>2023-12-01</td><td>100</td></tr><tr><td>101</td><td>2023-12-02</td><td>150</td></tr><tr><td>102</td><td>2023-12-01</td><td>200</td></tr><tr><td>102</td><td>2023-12-02</td><td>250</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "2023-12-01",
         100
        ],
        [
         "101",
         "2023-12-02",
         150
        ],
        [
         "102",
         "2023-12-01",
         200
        ],
        [
         "102",
         "2023-12-02",
         250
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# casting date column from string to date format\n",
    "df = df.withColumn(\"date\", to_date(col('date')))\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1df3259-942c-4e1d-b786-e0bdc0cc7544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>date</th><th>sales</th></tr></thead><tbody><tr><td>101</td><td>2023-12-02</td><td>150</td></tr><tr><td>102</td><td>2023-12-02</td><td>250</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "2023-12-02",
         150
        ],
        [
         "102",
         "2023-12-02",
         250
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "df.orderBy('product_id', 'date', ascending=[1,0]).dropDuplicates(subset=['product_id']).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba6779a-2575-42e8-8c90-6b4a408fe237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. While processing data from multiple files with inconsistent schemas, you need to merge them into a single DataFrame. How would you handle this inconsistency in PySpark?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0523c779-29b4-4bb2-be77-c04df56d31cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8b0c48-bc99-4f7b-873e-4c1e3b3f109a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('PySparkInterviewQuestions').getOrCreate()\n",
    "\n",
    "df = spark.read.format('parquet')\\\n",
    "        .option('mergeSchema', True)\\\n",
    "        .load('File/Data/datafiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dfb2af8-9435-457d-8361-764d0689ea36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. 🔍 Key Difference Between Spark and Hadoop MapReduce (Performance & Scalability)**\n",
    "\n",
    "#### ✅ Performance\n",
    "\n",
    "- **Apache Spark** is significantly faster than **Hadoop MapReduce** due to **in-memory processing**.\n",
    "  - **Spark** keeps intermediate data in **memory (RAM)**, avoiding slow disk I/O.\n",
    "  - **Hadoop MapReduce** writes intermediate data to **disk** after each map and reduce phase, increasing latency.\n",
    "- Spark also uses **advanced optimizations**:\n",
    "  - **Catalyst Optimizer** – Optimizes logical and physical query plans.\n",
    "  - **Tungsten Execution Engine** – Efficient memory management and byte code generation.\n",
    "\n",
    "#### ✅ Scalability\n",
    "\n",
    "- Both **Spark** and **Hadoop MapReduce** are **horizontally scalable** across clusters.\n",
    "- However, **Spark scales more efficiently** due to:\n",
    "  - Faster execution\n",
    "  - Lower resource usage for similar workloads\n",
    "- **Hadoop MapReduce** also scales well but often requires **more hardware** and **longer processing times** due to its disk-based nature.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚀 Why Spark Is Faster: Comparison Table\n",
    "\n",
    "| Feature                  | Spark                             | Hadoop MapReduce                 |\n",
    "|--------------------------|-----------------------------------|----------------------------------|\n",
    "| **Data Processing**      | In-memory                         | Disk-based                       |\n",
    "| **Execution Model**      | Directed Acyclic Graph (DAG)      | Map → Shuffle → Reduce phases    |\n",
    "| **Optimization**         | Catalyst + Tungsten               | No built-in query optimizer      |\n",
    "| **Speed**                | Up to 100× faster (in-memory ops) | Slower due to disk I/O           |\n",
    "| **Processing Type**      | Batch, Streaming, ML, Graph       | Batch only                       |\n",
    "\n",
    "---\n",
    "\n",
    "#### 📝 Summary\n",
    "\n",
    "> The **key performance difference** is that **Spark performs in-memory computation**, drastically reducing I/O overhead, while **Hadoop MapReduce relies on disk** between operations. This makes Spark **much faster and more efficient**, especially for **iterative and real-time tasks**.  \n",
    ">  \n",
    "> In terms of **scalability**, both scale well across clusters, but **Spark uses resources more efficiently**, offering better performance with less overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd04ee27-4740-4179-a838-9ab6d677f1e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. You are working with a real-time data pipeline, and you notice missing values in your streaming data Column - Category. How would you handle null or missing values in such a scenario?**\n",
    "\n",
    "**df_stream = spark.readStream.schema(\"id INT, value STRING\").csv(\"path/to/stream\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1c1396-59ba-4f2c-ad3e-3e527b466736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stream = spark.readStream.schema(\"id INT, value STRING\").csv(\"path/to/stream\")\n",
    "df  = df_stream.fillna({'Category': 'N/A'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e095f84-8394-4c2b-9293-8fdd00cabf83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. You need to calculate the total number of actions performed by users in a system. How would you calculate the top 5 most active users based on this information?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb952a7-f79a-4b9b-95c3-77540e8adec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>actions</th></tr></thead><tbody><tr><td>user1</td><td>5</td></tr><tr><td>user2</td><td>8</td></tr><tr><td>user3</td><td>2</td></tr><tr><td>user4</td><td>10</td></tr><tr><td>user2</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "user1",
         5
        ],
        [
         "user2",
         8
        ],
        [
         "user3",
         2
        ],
        [
         "user4",
         10
        ],
        [
         "user2",
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "actions",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"user1\", 5), (\"user2\", 8), (\"user3\", 2), (\"user4\", 10), (\"user2\", 3)]\n",
    "columns = [\"user_id\", \"actions\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73bc0513-45e6-4cdb-bab5-1aa1ccea5026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>total_actions</th></tr></thead><tbody><tr><td>user2</td><td>11</td></tr><tr><td>user4</td><td>10</td></tr><tr><td>user1</td><td>5</td></tr><tr><td>user3</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "user2",
         11
        ],
        [
         "user4",
         10
        ],
        [
         "user1",
         5
        ],
        [
         "user3",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_actions",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Groups actions by user_id, sums total actions per user, sorts by total actions descending, and selects top 5 users\n",
    "df = df.groupBy('user_id')\\\n",
    "        .agg(sum('actions').alias('total_actions'))\\\n",
    "        .orderBy('total_actions', ascending=False)\\\n",
    "        .limit(5)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb72827-03a2-40dc-b576-8960040beef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6. While processing sales transaction data, you need to identify the most recent transaction for each customer. How would you approach this task?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c211133-a3af-4da1-8577-29b5716970cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>transaction_date</th><th>sales</th></tr></thead><tbody><tr><td>cust1</td><td>2023-12-01</td><td>100</td></tr><tr><td>cust2</td><td>2023-12-02</td><td>150</td></tr><tr><td>cust1</td><td>2023-12-03</td><td>200</td></tr><tr><td>cust2</td><td>2023-12-04</td><td>250</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "cust1",
         "2023-12-01",
         100
        ],
        [
         "cust2",
         "2023-12-02",
         150
        ],
        [
         "cust1",
         "2023-12-03",
         200
        ],
        [
         "cust2",
         "2023-12-04",
         250
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transaction_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"cust1\", \"2023-12-01\", 100), (\"cust2\", \"2023-12-02\", 150),\n",
    "        (\"cust1\", \"2023-12-03\", 200), (\"cust2\", \"2023-12-04\", 250)]\n",
    "columns = [\"customer_id\", \"transaction_date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6fdf157-b20c-41b6-9253-21e39caef439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c704353-5762-4e33-b5fc-b972df05cadf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----+----+\n|customer_id|transaction_date|sales|flag|\n+-----------+----------------+-----+----+\n|      cust1|      2023-12-03|  200|   1|\n|      cust2|      2023-12-04|  250|   1|\n+-----------+----------------+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "# convert transaction_date from string to date type\n",
    "df = df.withColumn('transaction_date', col('transaction_date').cast(DateType()))\n",
    "# rank the transactions by date for each customer\n",
    "df = (\n",
    "    df.withColumn(\n",
    "        'flag',\n",
    "        dense_rank().over(\n",
    "            Window.partitionBy('customer_id').orderBy(desc('transaction_date'))\n",
    "        )\n",
    "    )\n",
    "    .filter(col('flag') == 1)\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b4992b-d437-4ee3-929b-5f7d37193983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**7. You need to identify customers who haven’t made any purchases in the last 30 days. How would you filter such customers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d32aa41-b8a6-4da0-a1b1-b13ad0e2e7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>last_purchase_date</th></tr></thead><tbody><tr><td>cust1</td><td>2025-09-01</td></tr><tr><td>cust2</td><td>2024-11-20</td></tr><tr><td>cust3</td><td>2024-11-25</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "cust1",
         "2025-09-01"
        ],
        [
         "cust2",
         "2024-11-20"
        ],
        [
         "cust3",
         "2024-11-25"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_purchase_date",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"cust1\", \"2025-09-01\"), (\"cust2\", \"2024-11-20\"), (\"cust3\", \"2024-11-25\")]\n",
    "columns = [\"customer_id\", \"last_purchase_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "096fafad-0029-415d-b5dd-24041cb484f7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757862200395}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>last_purchase_date</th><th>gap</th></tr></thead><tbody><tr><td>cust2</td><td>2024-11-20</td><td>310</td></tr><tr><td>cust3</td><td>2024-11-25</td><td>305</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "cust2",
         "2024-11-20",
         310
        ],
        [
         "cust3",
         "2024-11-25",
         305
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_purchase_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "gap",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert last_purchase_date from string to date type\n",
    "df = df.withColumn('last_purchase_date', to_date('last_purchase_date'))\n",
    "# calculate the gap between last_purchase_date and current_date\n",
    "df = df.withColumn('gap', datediff(current_date(), 'last_purchase_date')).filter(col('gap') > 30)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39a464f-2d11-440e-ace2-b3a4016fe62b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**8. While analyzing customer reviews, you need to identify the most frequently used words in the feedback. How would you implement this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4f6201-d5dd-4bbb-90fe-b991767eb4f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>feedback</th></tr></thead><tbody><tr><td>customer1</td><td>The product is great</td></tr><tr><td>customer2</td><td>Great product, fast delivery</td></tr><tr><td>customer3</td><td>Not bad, could be better</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "customer1",
         "The product is great"
        ],
        [
         "customer2",
         "Great product, fast delivery"
        ],
        [
         "customer3",
         "Not bad, could be better"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "feedback",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"customer1\", \"The product is great\"), (\"customer2\", \"Great product, fast delivery\"), (\"customer3\", \"Not bad, could be better\")]\n",
    "columns = [\"customer_id\", \"feedback\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6fed502-7926-4ab1-88ed-b9a8ab200677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>feedback</th><th>word_count</th></tr></thead><tbody><tr><td>product</td><td>1</td></tr><tr><td>great</td><td>2</td></tr><tr><td>the</td><td>1</td></tr><tr><td>is</td><td>1</td></tr><tr><td>product,</td><td>1</td></tr><tr><td>fast</td><td>1</td></tr><tr><td>delivery</td><td>1</td></tr><tr><td>better</td><td>1</td></tr><tr><td>be</td><td>1</td></tr><tr><td>could</td><td>1</td></tr><tr><td>bad,</td><td>1</td></tr><tr><td>not</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product",
         1
        ],
        [
         "great",
         2
        ],
        [
         "the",
         1
        ],
        [
         "is",
         1
        ],
        [
         "product,",
         1
        ],
        [
         "fast",
         1
        ],
        [
         "delivery",
         1
        ],
        [
         "better",
         1
        ],
        [
         "be",
         1
        ],
        [
         "could",
         1
        ],
        [
         "bad,",
         1
        ],
        [
         "not",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "feedback",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "word_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert feedback to lowercase and split into words\n",
    "df1 = df.withColumn('feedback', lower('feedback'))\\\n",
    "        .withColumn('feedback', explode(split('feedback', ' ')))\n",
    "# group by feedback and count the number of occurrences\n",
    "df_grp = df1.groupBy('feedback').agg(count('feedback').alias('word_count'))\n",
    "\n",
    "df_grp.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab76afc0-f0a2-4ca7-919b-b104028fc37c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**9. You need to calculate the cumulative sum of sales over time for each product. How would you approach this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb121df-32c6-4bb2-a31b-7a7bf68124ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>date</th><th>sales</th></tr></thead><tbody><tr><td>product1</td><td>2023-12-01</td><td>100</td></tr><tr><td>product2</td><td>2023-12-02</td><td>200</td></tr><tr><td>product1</td><td>2023-12-03</td><td>150</td></tr><tr><td>product2</td><td>2023-12-04</td><td>250</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         "2023-12-01",
         100
        ],
        [
         "product2",
         "2023-12-02",
         200
        ],
        [
         "product1",
         "2023-12-03",
         150
        ],
        [
         "product2",
         "2023-12-04",
         250
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-02\", 200), (\"product1\", \"2023-12-03\", 150), (\"product2\", \"2023-12-04\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d0c7f93-4802-441a-bdfd-63942c827ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>date</th><th>sales</th><th>cumsum</th></tr></thead><tbody><tr><td>product1</td><td>2023-12-01</td><td>100</td><td>100</td></tr><tr><td>product1</td><td>2023-12-03</td><td>150</td><td>250</td></tr><tr><td>product2</td><td>2023-12-02</td><td>200</td><td>200</td></tr><tr><td>product2</td><td>2023-12-04</td><td>250</td><td>450</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         "2023-12-01",
         100,
         100
        ],
        [
         "product1",
         "2023-12-03",
         150,
         250
        ],
        [
         "product2",
         "2023-12-02",
         200,
         200
        ],
        [
         "product2",
         "2023-12-04",
         250,
         450
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cumsum",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = df.withColumn('date', to_date('date'))\n",
    "# Calculates the cumulative sum of sales for each product over time, ordered by date\n",
    "df1 = df1.withColumn('cumsum', sum('sales').over(Window.partitionBy('product_id').orderBy('date')))\n",
    "\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996523c2-691b-49f6-a529-6323cc4176a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**10. While preparing a data pipeline, you notice some duplicate rows in a dataset. How would you remove the duplicates without affecting the original order?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fdf097-73fb-4b41-90a6-90a36346402f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th></tr></thead><tbody><tr><td>John</td><td>25</td></tr><tr><td>Jane</td><td>30</td></tr><tr><td>John</td><td>25</td></tr><tr><td>Alice</td><td>22</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John",
         25
        ],
        [
         "Jane",
         30
        ],
        [
         "John",
         25
        ],
        [
         "Alice",
         22
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"John\", 25), (\"Jane\", 30), (\"John\", 25), (\"Alice\", 22)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fada82a7-b19c-48c9-915c-b29e6e973346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>rowflag</th></tr></thead><tbody><tr><td>Alice</td><td>22</td><td>1</td></tr><tr><td>Jane</td><td>30</td><td>1</td></tr><tr><td>John</td><td>25</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         22,
         1
        ],
        [
         "Jane",
         30,
         1
        ],
        [
         "John",
         25,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "rowflag",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = df.withColumn('rowflag', row_number().over(Window.partitionBy('name').orderBy('age'))).filter(col('rowflag') == 1)\n",
    "\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca3047a-ef62-411e-8893-338bcaa04d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**11. You are working with user activity data and need to calculate the average session duration per user. How would you implement this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d1c78e-8e3e-464d-aa59-b20d82fe66b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>session_date</th><th>duration</th></tr></thead><tbody><tr><td>user1</td><td>2023-12-01</td><td>50</td></tr><tr><td>user1</td><td>2023-12-02</td><td>60</td></tr><tr><td>user2</td><td>2023-12-01</td><td>45</td></tr><tr><td>user2</td><td>2023-12-03</td><td>75</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "user1",
         "2023-12-01",
         50
        ],
        [
         "user1",
         "2023-12-02",
         60
        ],
        [
         "user2",
         "2023-12-01",
         45
        ],
        [
         "user2",
         "2023-12-03",
         75
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "session_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "duration",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"user1\", \"2023-12-01\", 50), (\"user1\", \"2023-12-02\", 60), \n",
    "        (\"user2\", \"2023-12-01\", 45), (\"user2\", \"2023-12-03\", 75)]\n",
    "columns = [\"user_id\", \"session_date\", \"duration\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ae1a70-ef07-4e41-aa03-20b651cb4b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>avg_duration</th></tr></thead><tbody><tr><td>user1</td><td>55.0</td></tr><tr><td>user2</td><td>60.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "user1",
         55.0
        ],
        [
         "user2",
         60.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg_duration",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.groupBy('user_id').agg(avg('duration').alias('avg_duration'))\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313980a9-a22f-469e-959d-07f458c62420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**12. While analyzing sales data, you need to find the product with the highest sales for each month. How would you accomplish this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a480a34b-b80b-42e0-ab71-a742f067157d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>date</th><th>sales</th></tr></thead><tbody><tr><td>product1</td><td>2023-12-01</td><td>100</td></tr><tr><td>product2</td><td>2023-12-01</td><td>150</td></tr><tr><td>product1</td><td>2023-12-02</td><td>200</td></tr><tr><td>product2</td><td>2023-12-02</td><td>250</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         "2023-12-01",
         100
        ],
        [
         "product2",
         "2023-12-01",
         150
        ],
        [
         "product1",
         "2023-12-02",
         200
        ],
        [
         "product2",
         "2023-12-02",
         250
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-01\", 150), \n",
    "        (\"product1\", \"2023-12-02\", 200), (\"product2\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257810dd-0737-43b0-b4d3-c86cc279715a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 13. 🔍 What is the role of sparkcontext in pyspark?\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔑 Role of `SparkContext` in PySpark\n",
    "\n",
    "* **Entry point to Spark Core**\n",
    "  `SparkContext` is the **gateway** to all Spark functionality in the JVM.\n",
    "  It represents the connection between your Python process (driver) and the underlying **Spark cluster** (executors).\n",
    "\n",
    "* **Cluster communication**\n",
    "  It is responsible for talking to the **cluster manager** (like YARN, Mesos, Kubernetes, or Spark’s built-in standalone scheduler) and acquiring executor resources.\n",
    "\n",
    "* **Task scheduling**\n",
    "  Once executors are allocated, `SparkContext` coordinates how **tasks** (your transformations and actions) are distributed and executed in parallel across the cluster.\n",
    "\n",
    "* **RDD API support**\n",
    "  Historically, Spark was built around **RDDs** (Resilient Distributed Datasets).\n",
    "  `SparkContext` is still the object that creates RDDs directly. For example:\n",
    "\n",
    "  ```python\n",
    "  sc = SparkContext(\"local\", \"MyApp\")\n",
    "  rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "  ```\n",
    "\n",
    "* **Bridge for higher-level APIs**\n",
    "  When you use the newer **`SparkSession`** (introduced in Spark 2.0 for DataFrame/Dataset API), it internally holds a `SparkContext`.\n",
    "  👉 `spark.sparkContext` gives you access to it.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📊 Typical workflow today\n",
    "\n",
    "Most PySpark developers don’t explicitly create a `SparkContext` anymore — they use `SparkSession`:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "sc = spark.sparkContext   # access underlying SparkContext if needed\n",
    "```\n",
    "\n",
    "You only touch `sc` when:\n",
    "\n",
    "* You want to work with **RDDs directly** (rare, but useful for low-level transformations).\n",
    "* You need to check cluster info, for example:\n",
    "\n",
    "  ```python\n",
    "  print(sc.master)        # cluster manager URL\n",
    "  print(sc.appName)       # application name\n",
    "  print(sc.defaultParallelism)  # default number of partitions\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚡ In short:\n",
    "\n",
    "* `SparkContext` = low-level **engine connection & cluster interface**.\n",
    "* `SparkSession` = high-level **entry point for SQL/DataFrames**, built on top of `SparkContext`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc9020cf-44f9-45aa-a712-db6e52222b95",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758299904666}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date</th><th>product_id</th><th>sum_sales</th><th>ranking</th></tr></thead><tbody><tr><td>12</td><td>product2</td><td>400</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         12,
         "product2",
         400,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sum_sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ranking",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = df.withColumn('date', to_date('date'))\n",
    "\n",
    "df1 = df1.withColumn('date', month('date')).groupBy('date', 'product_id').agg(sum(\"sales\").alias('sum_sales'))\n",
    "\n",
    "df1 = df1.withColumn(\n",
    "        'ranking', \n",
    "        dense_rank().over(\n",
    "                Window.partitionBy('date').orderBy(col('sum_sales').desc())\n",
    "        )\n",
    ").filter(col('ranking') == 1)\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c8f1aa5-2bde-49a2-8bce-a2c045746473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 14. 🔍 Spark Architecture - Explain\n",
    "\n",
    "\n",
    "```pgsql\n",
    "            +----------------------+\n",
    "            |      Driver Program  |\n",
    "            |  (SparkContext, DAG) |\n",
    "            +-----------+----------+\n",
    "                        |\n",
    "               Job Scheduling / DAG\n",
    "                        |\n",
    "             +----------v----------+\n",
    "             |    Cluster Manager  |\n",
    "             |  (YARN, Mesos, etc) |\n",
    "             +----------+----------+\n",
    "                        |\n",
    "        +---------------+---------------+\n",
    "        |                               |\n",
    "+-------v--------+              +-------v--------+\n",
    "|    Executor    |              |    Executor    |\n",
    "|   (Worker 1)   |              |   (Worker 2)   |\n",
    "| - Tasks        |              | - Tasks        |\n",
    "| - Cache / RDDs |              | - Cache / RDDs |\n",
    "+----------------+              +----------------+\n",
    "```\n",
    "\n",
    "\n",
    "The entire process is orchestrated by the **Driver Node** and mediated by the **Cluster Manager**.\n",
    "\n",
    "### 1. Application Submission\n",
    "\n",
    "When an application (your PySpark code) is submitted, it first interfaces with the **Cluster Manager**. The Cluster Manager can be a system like Yarn Hadoop or a Spark standalone cluster.\n",
    "\n",
    "### 2. Driver Setup\n",
    "\n",
    "The Cluster Manager then performs the following crucial setup steps:\n",
    "\n",
    "*   **Driver Node Selection:** It selects one machine or node within the cluster (or outside, depending on the mode) to establish the **Driver Program**, which becomes the **Driver Node**. The Driver Node is regarded as the **brain** of the operation.\n",
    "*   **Spark Context Creation:** Within the Driver Node, the **Spark Context** is created. The Spark Context is the **starting point of Spark** and acts as a **bridge between the driver program and the cluster manager**. It is responsible for creating the connection to the cluster so Spark can access essential resources like CPU and memory for processing.\n",
    "\n",
    "### 3. Planning and Resource Allocation\n",
    "\n",
    "The Driver Program takes over the orchestration:\n",
    "\n",
    "*   **Analysis:** The Driver Program analyzes the submitted code to determine the necessary transformations that need to be applied.\n",
    "*   **Resource Request:** If the execution plan requires more resources, the Driver Program uses the **Spark Context** to communicate that information to the Cluster Manager (e.g., requesting additional worker machines or nodes).\n",
    "*   **Worker Assignment:** The Cluster Manager allocates the requested **Worker Nodes** to the application.\n",
    "\n",
    "### 4. Task Execution\n",
    "\n",
    "Finally, the execution of the code occurs:\n",
    "\n",
    "*   The Driver Node **communicates with the Worker Nodes**, passing on the required information and **guidelines (transformations)** they must follow to execute the work.\n",
    "*   The Worker Nodes then execute these parallel processing tasks.\n",
    "\n",
    "### Execution Modes\n",
    "\n",
    "The flow varies slightly based on where the Driver Node resides:\n",
    "\n",
    "| Execution Mode | Driver Node Location |\n",
    "| :--- | :--- |\n",
    "| **Cluster Mode** | The Driver Node is picked and resides **within the cluster**. |\n",
    "| **Client Mode** | The Driver Node resides **outside of the cluster** (e.g., the user's local machine or another VM). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "160b312b-3cde-42a5-8561-95f0afc971fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**13. You are working with a large Delta table that is frequently updated by multiple users. The data is stored in partitions, and sometimes updates can cause inconsistent reads due to concurrent transactions. How would you ensure ACID compliance and avoid data corruption in PySpark?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e274f9b3-7597-4385-aa80-fb96ee9e8a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**New data**\n",
    "\n",
    "df_new = spark.read.format('parquet').load('path')\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_tbl = DeltaTable.forPath('path')\n",
    "\n",
    "delta_tbl.alias('target').merge(df_new.alias('src'), \"src.id == target.id\")\\\n",
    "  .whenNotMatchedInsertAll()\\\n",
    "  .whenMatchedUpdateAll()\\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11526848-28ab-4755-97c8-7d7614ed8f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**14. You need to process a large dataset stored in PARQUET format and ensure that all columns have the right schema (Almost). How would you do this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c55e91d-e1f6-41c9-b16a-637ce1a5a938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new = spark.read.format('parquet')\\\n",
    "              .option('inferSchema', True)\\\n",
    "              .load('path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd4f0e1-62d1-408b-a1ea-f855b3849113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**15. You are reading a CSV file and need to handle corrupt records gracefully by skipping them. How would you configure this in PySpark?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6775877-5e8b-416a-8ae7-9b871a0dd959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new = spark.read.format('csv')\\\n",
    "            .option('mode', 'DROPMALFORMED')\\\n",
    "            .load('staging_location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f28346-9028-4fbf-a628-05f9cbd44eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 16. 🔍What is difference between RDDs, Dataframe and Dataset ?\n",
    "\n",
    "In PySpark, **RDDs (Resilient Distributed Datasets)**, **DataFrames**, and **Datasets** are different abstractions for working with distributed data. Here's a breakdown of their key differences:\n",
    "\n",
    "### 1. 🧩**RDD (Resilient Distributed Dataset)**\n",
    "\n",
    "* **Low-Level Abstraction**: RDDs are the most basic and low-level abstraction for distributed data in Spark.\n",
    "* **Immutable**: RDDs are immutable collections of objects distributed across a cluster.\n",
    "* **Fault Tolerance**: They offer fault tolerance via lineage, meaning Spark can recompute lost data from the original dataset.\n",
    "* **No Schema**: RDDs don't have a schema, so the data structure isn't enforced.\n",
    "* **Data Processing**: You need to manually handle the data transformations (like `map`, `flatMap`, `filter`, `reduce`).\n",
    "* **Performance**: Generally slower than DataFrames and Datasets because Spark doesn't optimize RDD operations.\n",
    "\n",
    "#### Use Case:\n",
    "\n",
    "* When you need fine-grained control over transformations and data processing.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "rdd.map(lambda x: x * 2).collect()\n",
    "```\n",
    "\n",
    "### 2. 📊**DataFrame**\n",
    "\n",
    "* **Higher-Level Abstraction**: DataFrames are a higher-level abstraction, built on top of RDDs, designed for working with structured data.\n",
    "* **Schema**: DataFrames are organized into columns with a schema (like a table in a relational database).\n",
    "* **Optimized**: Spark applies optimizations like Catalyst query optimization and Tungsten physical execution engine to DataFrames, making them more performant than RDDs.\n",
    "* **APIs**: DataFrames support SQL-like queries (using methods like `select`, `filter`, `groupBy`, etc.).\n",
    "* **Interoperability**: You can convert between DataFrames and RDDs easily.\n",
    "* **Lazy Evaluation**: DataFrame operations are lazily evaluated, meaning computation is triggered only when an action (e.g., `show()`, `collect()`) is performed.\n",
    "\n",
    "#### Use Case:\n",
    "\n",
    "* When working with structured data, especially when performance optimization is required (like for SQL operations).\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"ID\", \"Name\"])\n",
    "df.filter(df[\"ID\"] == 1).show()\n",
    "```\n",
    "\n",
    "### 3. 🛠️**Dataset**\n",
    "\n",
    "* **Type-Safe**: Datasets provide type safety, which is available only in the Scala API. However, in PySpark, Datasets are basically a DataFrame with additional type-safe operations available in languages like Scala (not directly in Python).\n",
    "* **Combination of RDDs and DataFrames**: Datasets combine the advantages of RDDs and DataFrames — they provide the strong typing and functional programming capabilities of RDDs, along with the optimization features of DataFrames.\n",
    "* **Only in Scala/Java**: Datasets are available in Scala and Java but are not a native concept in PySpark, so for PySpark, the closest equivalent is DataFrames.\n",
    "\n",
    "#### Use Case:\n",
    "\n",
    "* For Scala/Java users who need both type safety and optimization.\n",
    "\n",
    "#### Example (Scala):\n",
    "\n",
    "```scala\n",
    "val ds = Seq((1, \"Alice\"), (2, \"Bob\")).toDS()\n",
    "ds.filter($\"_1\" === 1).show()\n",
    "```\n",
    "\n",
    "### Key Differences in Summary:\n",
    "\n",
    "| Feature              | **RDD**                              | **DataFrame**                      | **Dataset** (Scala/Java)          |\n",
    "| -------------------- | ------------------------------------ | ---------------------------------- | --------------------------------- |\n",
    "| **Type Safety**      | No                                   | No                                 | Yes (only in Scala/Java)          |\n",
    "| **Schema**           | No schema                            | Schema (structured data)           | Schema + Type-safe                |\n",
    "| **Optimization**     | No optimizations                     | Optimized (Catalyst, Tungsten)     | Optimized (Catalyst, Tungsten)    |\n",
    "| **API**              | Functional programming (map, reduce) | SQL-like (select, filter, groupBy) | SQL-like + functional programming |\n",
    "| **Fault Tolerance**  | Yes (via lineage)                    | Yes (via DataFrame lineage)        | Yes                               |\n",
    "| **Language Support** | Python, Scala, Java, R               | Python, Scala, Java, R             | Scala, Java                       |\n",
    "\n",
    "### When to Use Which:\n",
    "\n",
    "* **RDD**: Use when you need fine control and when performance isn't a concern. Ideal for low-level transformations or working with unstructured data.\n",
    "* **DataFrame**: Use when you have structured data and need to leverage SQL-like queries and optimizations.\n",
    "* **Dataset**: Use in Scala or Java when you need both the power of RDDs and the optimization of DataFrames, along with type safety.\n",
    "\n",
    "In PySpark, you'll most often be working with **DataFrames** since **Datasets** are not available in Python, and **RDDs** are mainly used for lower-level operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fef9155-7544-4ea5-9d18-c949b062b08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **17. 🔍 What is Query Optimization ?**\n",
    "\n",
    "Query optimization, also known as query planning, is the process by which Spark optimizes a query to determine the most efficient way to execute the code on multiple machines. This is a fundamental concept, as it is one of the reasons Spark is faster than Hadoop MapReduce.\n",
    "\n",
    "Query optimization is facilitated by Spark's internal mechanisms, notably the **Catalyst Optimizer**, and involves converting the written code through several stages before execution:\n",
    "\n",
    "### 1. The Code\n",
    "\n",
    "The process begins when you write code in a PySpark notebook or application.\n",
    "\n",
    "### 2. Logical Plan\n",
    "\n",
    "The code is first converted into a Logical Plan.\n",
    "\n",
    "*   **Purpose:** The Logical Plan determines the optimal order in which transformations should be performed. It analyzes the defined transformations (such as `select`, `where`, and `join`).\n",
    "*   **Optimization:** Spark does not necessarily execute all transformations in the exact order you specify. Instead, it smartly decides the execution order to make the query efficient and run faster.\n",
    "*   **Example:** If you apply a join and then select two columns, the logical plan might choose to select the columns first to immediately reduce the data size before executing the join.\n",
    "*   **Mechanism:** The Logical Plan is created by the **Catalyst Optimizer**.\n",
    "\n",
    "### 3. Physical Plan\n",
    "\n",
    "Once the Logical Plan is finalized, it is converted into a Physical Plan.\n",
    "\n",
    "*   **Purpose:** The Physical Plan determines the concrete execution strategy for the transformations defined in the Logical Plan.\n",
    "*   **Cost Comparison:** It compares the cost of the transformation (e.g., a join) against a specific cost model.\n",
    "*   **Strategy Selection:** It picks the **least expensive transformation** type for the given scenario. For example, if a join is required, the Physical Plan chooses the best specific join type, such as sort merge join, shuffle join, or hash join.\n",
    "\n",
    "### 4. Conversion to RDDs\n",
    "\n",
    "Finally, the chosen Physical Plan is converted into **Resilient Distributed Data Sets (RDDs)**.\n",
    "\n",
    "*   **Execution:** All transformations (whether from DataFrames or DataSets) are ultimately converted into RDDs so that the data can be distributed among nodes and executed in parallel across the cluster.\n",
    "\n",
    "This optimization capability is a key distinction between DataFrames/DataSets and RDDs; pure RDDs are \"really, really slow\" because they **cannot perform any query planning** or optimizations, while DataFrames enable users to perform query planning and various optimizations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201267ea-5c57-4c90-8f8b-1644edc3ddb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 18. 🔍Tell us about SPARK SESSION.\n",
    "\n",
    "### ✅ Spark Session in PySpark\n",
    "\n",
    "The **Spark Session** is a core component in PySpark, serving as the modern entry point for interacting with Spark functionality.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Definition and Role\n",
    "\n",
    "The Spark Session is the **entry point for Spark**. It acts as a single, unified point of entry that allows developers to use various Spark functionalities.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. History and Context (Spark Session vs. Spark Context)\n",
    "\n",
    "A common point of confusion arises because the sources mention two different entry points: **Spark Session** and **Spark Context**.\n",
    "\n",
    "* **Spark Context** was the **older entry point** for Spark.\n",
    "* The **Spark Session** is the **newer entry point**, introduced with **Spark 2.0** and available in all versions released after Spark 2.0.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Consolidation of Functionality\n",
    "\n",
    "The major advantage and reason for the introduction of the Spark Session was to consolidate multiple entry points previously needed before Spark 2.0.\n",
    "\n",
    "Previously, users needed three different types of entry points:\n",
    "\n",
    "1. **Spark Context**\n",
    "2. **SQL Context**\n",
    "3. **Hive Context**\n",
    "\n",
    "The Spark Session **includes everything** that Spark Context does, plus it offers **support for SQL operations**. It internally manages the Spark Context, SQL Context, and Hive Context, meaning you do not need to explicitly create them.\n",
    "\n",
    "This allows developers to manage all entry points together **under one hood**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Current Usage\n",
    "\n",
    "It is generally **recommended** that developers use **Spark Session** after Spark 2.0. Although it is still possible to use Spark Context to support legacy applications created by companies, using Spark Session is preferred because:\n",
    "\n",
    "* It is compatible\n",
    "* It streamlines the process\n",
    "* It manages all necessary contexts internally\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746b5bfa-81a2-421a-b556-52ecec3214e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 19. 🔍Difference Between Wide Transformations and Narrow Transformations in PySpark\n",
    "\n",
    "✅ The difference between **Wide Transformations** and **Narrow Transformations** is a fundamental concept in PySpark, revolving around the necessity of **data shuffling** across the nodes in a cluster. Since Spark is primarily used for transformations, understanding this distinction is crucial for performance optimization.\n",
    "\n",
    "---\n",
    "\n",
    "#### Narrow Transformation\n",
    "\n",
    "| Feature            | Description                                                                                                                                                                                                                                                                                                                                                                     | Source |\n",
    "| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ |\n",
    "| **Data Shuffling** | **No data shuffling** occurs among the nodes.                                                                                                                                                                                                                                                                                                                                   |        |\n",
    "| **Processing**     | The machine/node is **independent** and \"doesn't need to bother data sitting in other machine\". Transformations are applied using only the data residing within the local partition.                                                                                                                                                                                            |        |\n",
    "| **Efficiency**     | Narrow transformations are generally **more efficient** because they avoid the expensive process of data shuffling across the network.                                                                                                                                                                                                                                          |        |\n",
    "| **Example**        | A **Filter** transformation is a basic and very popular example. If you filter for `ID greater than three`, the local machine can simply check its data and return the result without needing to consult other nodes. For instance, if Node 1 has IDs 1, 2, 5, 9, it returns 5 and 9; and if Node 2 has IDs 3, 6, 7, it returns 6 and 7. No data is shuffled between the nodes. |        |\n",
    "\n",
    "---\n",
    "\n",
    "#### Wide Transformation\n",
    "\n",
    "| Feature            | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Source |\n",
    "| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ |\n",
    "| **Data Shuffling** | **Data shuffling will be there** between the machines or nodes. Data needs to be shuffled to give the correct final output.                                                                                                                                                                                                                                                                                                                                                                                                          |        |\n",
    "| **Processing**     | Processing requires gathering data from different partitions across multiple nodes.                                                                                                                                                                                                                                                                                                                                                                                                                                                  |        |\n",
    "| **Efficiency**     | Wide transformations are **inherently slower and more expensive** because shuffling involves moving large amounts of data across the network.                                                                                                                                                                                                                                                                                                                                                                                        |        |\n",
    "| **Example**        | A **Group By** transformation is a very good example of a wide transformation. If you apply a `Group by` on an `ID` column, and the same ID exists on multiple machines, the system needs to **shuffle** (gather) all related records to the same machine to complete the aggregation. For example, if ID '1' exists on Machine A, Machine B, and Machine C, the system must move all records associated with ID '1' to one machine before the aggregation (like summing the price associated with ID '1') can be correctly applied. |        |\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Narrow Transformation Example: `filter()`\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NarrowTransformation\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 29), (\"Bob\", 31), (\"Cathy\", 25)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Narrow transformation: filter (each output partition depends only on input partition)\n",
    "filtered_df = df.filter(df.Age > 28)\n",
    "\n",
    "filtered_df.show()\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "* Here, `filter()` is a narrow transformation — no shuffle needed, each partition filters its own data.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Wide Transformation Example: `groupBy()`\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WideTransformation\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", \"Math\", 85), (\"Bob\", \"Math\", 90), (\"Alice\", \"English\", 95), (\"Bob\", \"English\", 80)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Subject\", \"Score\"])\n",
    "\n",
    "# Wide transformation: groupBy causes shuffle, since data must be grouped across partitions\n",
    "grouped_df = df.groupBy(\"Name\").avg(\"Score\")\n",
    "\n",
    "grouped_df.show()\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "* `groupBy()` triggers a shuffle because Spark needs to group all records with the same key (Name) together, which might involve moving data between partitions.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7166e517-5f9c-4e6b-813d-94dd35db592c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 20. 🔍`COALESCE()` vs. `REPARTITION()` in PySpark\n",
    "\n",
    "✅ The source material discusses the functions `COALESCE()` and `REPARTITION()` as very handy functions in PySpark. They are primarily used for managing the number and size of data partitions across the cluster.\n",
    "\n",
    "Here is a breakdown of the use and characteristics of each function:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. `COALESCE()`\n",
    "\n",
    "The primary use of the `COALESCE()` function is to **reduce the number of partitions**.\n",
    "\n",
    "* **Purpose:**\n",
    "  `COALESCE()` is used to combine existing partitions into fewer partitions. It is directly linked to the `OPTIMIZE` command, which also involves coalescing partitions to create fewer, bigger partitions.\n",
    "\n",
    "* **Best Practice / Rule of Thumb:**\n",
    "  When dealing with Big Data, it is generally considered easier to read data from **bigger and fewer partitions** rather than smaller and numerous ones.\n",
    "\n",
    "* **Data Shuffling:**\n",
    "  A key characteristic of `COALESCE()` is that it **does not require shuffling** the data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `REPARTITION()`\n",
    "\n",
    "The `REPARTITION()` function provides flexibility in managing partitions, allowing the user to increase or decrease the number of partitions.\n",
    "\n",
    "* **Purpose:**\n",
    "  `REPARTITION()` is used to change the number of partitions.\n",
    "\n",
    "  * **Increasing Partitions:**\n",
    "    If you have two large partitions and are experiencing memory issues, you can use `REPARTITION()` to distribute the data among more machines/nodes.\n",
    "\n",
    "  * **Syntax Example:**\n",
    "    You can simply use `df.repartition(n)` and provide any number (e.g., 3, 4, 5, 6) to create that number of partitions.\n",
    "\n",
    "* **Data Shuffling:**\n",
    "  Unlike `COALESCE()`, `REPARTITION()` **requires shuffling** the data. This is because it needs to break up and redistribute the data into the specified number of partitions.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔄 Summary Table\n",
    "\n",
    "| Feature             | `COALESCE()`                                                                         | `REPARTITION()`                                                                              |\n",
    "| ------------------- | ------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------- |\n",
    "| **Primary Use**     | **Always reduces** the number of partitions.                                         | Can **increase or decrease** the number of partitions.                                       |\n",
    "| **Data Shuffling**  | **Does not require shuffling** the data.                                             | **Requires shuffling** the data.                                                             |\n",
    "| **Common Scenario** | Combining many small partitions into fewer large ones (e.g., for optimized reading). | Redistributing data among more nodes (e.g., to resolve memory issues with large partitions). |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ca48a0-8755-4214-9ac1-621b335f3907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 21. 🔍When to use `CACHE()` and `PERSIST()` in PySpark? What's the difference ?\n",
    "\n",
    "✅ The core use of both the `CACHE()` and `PERSIST()` functions is to **store the intermediate results** of a DataFrame. This prevents Spark from having to **recompute** the DataFrame from scratch every time it is used.\n",
    "\n",
    "You should use either `CACHE()` or `PERSIST()` **when you need to use the result or intermediate results multiple times**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 1. `CACHE()`\n",
    "\n",
    "* **Relationship to `PERSIST()`:**\n",
    "  `CACHE()` is essentially a special, simplified version of the `PERSIST()` function.\n",
    "\n",
    "* **Storage Level:**\n",
    "  When you use `df.cache()`, it is equivalent to calling:\n",
    "\n",
    "  ```python\n",
    "  df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "  ```\n",
    "\n",
    "* **Convenience:**\n",
    "  This specific storage level (`MEMORY_AND_DISK`) is so commonly used that Spark provides `cache()` as a **shortcut**, requiring **no arguments**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔸 2. `PERSIST()`\n",
    "\n",
    "* **Functionality:**\n",
    "  `PERSIST()` is used when you want to **customize how and where** Spark stores intermediate results.\n",
    "\n",
    "* **Flexibility (Storage Options):**\n",
    "  With `PERSIST()`, you can choose from several storage levels:\n",
    "\n",
    "  * `MEMORY_AND_DISK` *(default, same as `CACHE()`)*\n",
    "  * `MEMORY_ONLY`\n",
    "  * `DISK_ONLY`\n",
    "  * `MEMORY_AND_DISK_SER`\n",
    "  * `MEMORY_ONLY_SER`\n",
    "\n",
    "  Example:\n",
    "\n",
    "  ```python\n",
    "  from pyspark import StorageLevel\n",
    "  df.persist(StorageLevel.MEMORY_ONLY)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧾 Summary Table\n",
    "\n",
    "| Feature           | `CACHE()`                                            | `PERSIST()`                                                 |\n",
    "| ----------------- | ---------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| **Primary Goal**  | Stores intermediate results for reuse.               | Stores intermediate results for reuse.                      |\n",
    "| **Storage Level** | Fixed to **`StorageLevel.MEMORY_AND_DISK`**          | User-defined (e.g., `MEMORY_ONLY`, `DISK_ONLY`, etc.)       |\n",
    "| **Syntax**        | No arguments (e.g., `df.cache()`)                    | Requires a storage level argument (e.g., `df.persist(...)`) |\n",
    "| **Relationship**  | Shortcut for `persist(StorageLevel.MEMORY_AND_DISK)` | More flexible, base function for storage configuration      |\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ When to Use Which?\n",
    "\n",
    "* Use **`CACHE()`** if:\n",
    "\n",
    "  * You’re okay with the default behavior (store in memory and spill to disk if needed).\n",
    "  * You want **simplicity**.\n",
    "\n",
    "* Use **`PERSIST()`** if:\n",
    "\n",
    "  * You need **fine-grained control** over how data is stored.\n",
    "  * You want to **optimize for memory or disk usage** based on your cluster's resources.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48735995-5fa7-4f3f-b0c3-1b2d8663f1bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 22. 🔍What is the importance of PARTITIONS in PYSPARK ?\n",
    "\n",
    "#### 📦 Importance of Partitions in PySpark\n",
    "\n",
    "✅ The importance of partitions in PySpark is fundamental to achieving **high performance** and **efficiency** in processing large datasets. Partitions enable Spark to utilize **parallel processing** capabilities effectively.\n",
    "\n",
    "Here is a detailed explanation of why partitions matter:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. ⚙️ Enabling Parallelism (MPP)\n",
    "\n",
    "The primary reason for using partitions is to achieve **parallelism**.\n",
    "\n",
    "* When you talk about partitions in PySpark, you are asking Spark to **perform parallelism**.\n",
    "* This concept is also referred to using the technical term **MPP (Massive Parallel Processing)**.\n",
    "* Partitions work by taking a massive dataset and **distributing it into smaller chunks** across multiple machines or nodes.\n",
    "* These machines then process the data **in parallel**, improving speed and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 🚀 Promoting Query Optimization\n",
    "\n",
    "Partitions are crucial for optimizing how data is **read and processed**.\n",
    "\n",
    "* When you write data, it is more efficient to **read from fewer, larger partitions** rather than from many small ones.\n",
    "* This is a common **rule of thumb** in Big Data processing.\n",
    "* Partitions can be created during the **write process** using functions like `partitionBy`, which:\n",
    "\n",
    "  * Organize the data layout.\n",
    "  * **Speed up queries** by limiting the amount of data read.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 📈 Impact on Performance Management\n",
    "\n",
    "Partitions directly impact **execution speed**, **resource management**, and **cluster efficiency**:\n",
    "\n",
    "* **Handling Large Data:**\n",
    "\n",
    "  * Large partitions can cause **memory issues**.\n",
    "  * Use `REPARTITION()` to **increase partition count** and better distribute load across the cluster.\n",
    "\n",
    "* **Optimizing Small Partitions:**\n",
    "\n",
    "  * Use `COALESCE()` to **reduce partition count**, often after filtering or joins, to avoid overhead from too many tiny partitions.\n",
    "  * Typically combined with `OPTIMIZE` for **efficient reads**.\n",
    "\n",
    "* **Delta Lake Optimization:**\n",
    "\n",
    "  * The `OPTIMIZE` command coalesces partitions into fewer, larger ones.\n",
    "  * Combine with `Z-ORDER BY` to **sort data within partitions**.\n",
    "  * Enables **data skipping**, where Spark **skips partitions** that don’t contain relevant data, drastically improving performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Summary\n",
    "\n",
    "Partitions are **fundamental** because they determine how data is:\n",
    "\n",
    "* **Divided** across the cluster\n",
    "* **Processed** in parallel\n",
    "* **Stored** and **read** efficiently\n",
    "\n",
    "Proper partitioning leads to:\n",
    "\n",
    "* Faster execution\n",
    "* Lower memory pressure\n",
    "* Better scalability\n",
    "* Smarter query planning (via partition pruning & data skipping)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6744cc9c-caf5-41c8-8129-b677b051982f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**22. You have a dataset containing the names of employees and their departments. You need to find the department with the most employees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06e5e20-caec-40d7-9f14-5b414b0402ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_name</th><th>department</th></tr></thead><tbody><tr><td>Alice</td><td>HR</td></tr><tr><td>Bob</td><td>Finance</td></tr><tr><td>Charlie</td><td>HR</td></tr><tr><td>David</td><td>Engineering</td></tr><tr><td>Eve</td><td>Finance</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         "HR"
        ],
        [
         "Bob",
         "Finance"
        ],
        [
         "Charlie",
         "HR"
        ],
        [
         "David",
         "Engineering"
        ],
        [
         "Eve",
         "Finance"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"Alice\", \"HR\"), (\"Bob\", \"Finance\"), (\"Charlie\", \"HR\"), (\"David\", \"Engineering\"), (\"Eve\", \"Finance\")]\n",
    "columns = [\"employee_name\", \"department\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "985b8700-1fc0-4fcd-876c-5c181bea5ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>total_emp</th></tr></thead><tbody><tr><td>HR</td><td>2</td></tr><tr><td>Finance</td><td>2</td></tr><tr><td>Engineering</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "HR",
         2
        ],
        [
         "Finance",
         2
        ],
        [
         "Engineering",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_emp",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = df.groupBy('department')\\\n",
    "        .agg(count('employee_name').alias('total_emp'))\\\n",
    "        .sort('total_emp', ascending=False)\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1118756b-9c53-4b4e-aa76-327b35206b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**23. While processing sales data, you need to classify each transaction as either 'High' or 'Low' based on its amount. How would you achieve this using a when condition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ee0039-d5f6-4345-b952-ff46343be91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>sales</th></tr></thead><tbody><tr><td>product1</td><td>100</td></tr><tr><td>product2</td><td>300</td></tr><tr><td>product3</td><td>50</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         100
        ],
        [
         "product2",
         300
        ],
        [
         "product3",
         50
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 300), (\"product3\", 50)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2e842f-235d-44d6-a777-da0fd1c6c83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>sales</th><th>price_cat</th></tr></thead><tbody><tr><td>product1</td><td>100</td><td>High</td></tr><tr><td>product2</td><td>300</td><td>High</td></tr><tr><td>product3</td><td>50</td><td>Low</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         100,
         "High"
        ],
        [
         "product2",
         300,
         "High"
        ],
        [
         "product3",
         50,
         "Low"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "price_cat",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 =df.withColumn('price_cat', when(col('sales')>50, 'High').otherwise('Low'))\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8ff96e-1717-4d5c-a6fd-e363c9678b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**24. While analyzing a large dataset, you need to create a new column that holds a timestamp of when the record was processed. How would you implement this and what can be the best USE CASE?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d95874c-7ef1-4c38-9d10-8869914c8d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>sales</th></tr></thead><tbody><tr><td>product1</td><td>100</td></tr><tr><td>product2</td><td>200</td></tr><tr><td>product3</td><td>300</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         100
        ],
        [
         "product2",
         200
        ],
        [
         "product3",
         300
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 200), (\"product3\", 300)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90a9593-fab6-4a88-857d-56e311fd337a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>sales</th><th>processed_time</th></tr></thead><tbody><tr><td>product1</td><td>100</td><td>2025-09-26T18:00:15.285Z</td></tr><tr><td>product2</td><td>200</td><td>2025-09-26T18:00:15.285Z</td></tr><tr><td>product3</td><td>300</td><td>2025-09-26T18:00:15.285Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         100,
         "2025-09-26T18:00:15.285Z"
        ],
        [
         "product2",
         200,
         "2025-09-26T18:00:15.285Z"
        ],
        [
         "product3",
         300,
         "2025-09-26T18:00:15.285Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "processed_time",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.withColumn('processed_time', current_timestamp()).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6015b2da-3990-4179-afe4-f66269e3f692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🛠️ Implementation\n",
    "\n",
    "The implementation involves using the built-in PySpark function `current_timestamp()` to capture the exact time of processing and adding it as a new column using `withColumn()`.\n",
    "\n",
    "1.  🧠 **Function Used:** The function required is `current_timestamp()` because the interviewer wants a detailed time.\n",
    "2.  💻 **Implementation Code:** You would use the following approach (assuming the new column is named `process_time`):\n",
    "    ```python\n",
    "    DF = DF.withColumn(\"process_time\", current_timestamp())\n",
    "    ```\n",
    "3.  ⏱️ **Output Detail:** When implemented, this provides a detailed timestamp, showing the year, month, day, and the time down to milliseconds and seconds, demonstrating exactly **at what time these records were actually processed**.\n",
    "\n",
    "### 🚀 Best Use Case\n",
    "\n",
    "The best use case for creating a column that holds the processing timestamp relates directly to scenarios where auditing, tracking, or managing data evolution is necessary.\n",
    "\n",
    "*   🧩 **Scenario:** The most common and best use case is when dealing with **Slowly Changing Dimensions (SCD)**.\n",
    "*   🎯 **Purpose:** In the scenario of building a data warehouse or dealing with SCDs, you need to tell **at what time this record was changed**, modified, or created.\n",
    "*   📁 **Specific Columns:** This functionality is often used to create audit columns such as **`create_time`** and **`modify_time`**.\n",
    "*   🏗️ **Context:** By applying the `current_timestamp()` function, you can demonstrate that you used this approach when **building a data warehouse** or dealing with Slowly Changing Dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ad21b8-d454-4e6e-a815-52c3af684abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**25. You need to register this PySpark DataFrame as a temporary SQL object and run a query on it. How would you achieve this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a19a0ca-e434-4e41-bd79-1938d25bd579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>sales</th></tr></thead><tbody><tr><td>product1</td><td>100</td></tr><tr><td>product2</td><td>200</td></tr><tr><td>product3</td><td>300</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         100
        ],
        [
         "product2",
         200
        ],
        [
         "product3",
         300
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 200), (\"product3\", 300)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b105ee2-00e3-4dd5-932a-c3da58da3916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('tempsqldf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603f4599-a256-4acf-9bf2-ef1f8045bdab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[product_id: string, sales: bigint]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM tempsqldf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c485e3-88d7-4f35-b40b-5d73f319cc94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>sales</th></tr></thead><tbody><tr><td>product1</td><td>100</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         100
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "product_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "sales",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 186
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM tempsqldf WHERE sales = 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b813aec2-fbdc-4390-b326-cf47e16adfee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**26. You need to register this PySpark DataFrame as a temporary SQL object and run a query on it (FROM DIFFERENT NOTEBOOKS AS WELL)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef0e0ec-c77d-4eb6-9bcf-dd0158325e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "df.createOrReplaceGlobalTempView('globalview')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b6ef69-5728-4e4c-bca5-ccff5fe7bf5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SELECT * FROM global_temp.globalview;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e844ca9-4f82-4499-a872-fae8b79a24fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**27. You need to query data from a PySpark DataFrame using SQL, but the data includes a nested structure. How would you flatten the data for easier querying?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af07d8e5-265d-4c53-b5c0-766ae793eaf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>product_info</th></tr></thead><tbody><tr><td>product1</td><td>Map(price -> 100, quantity -> 2)</td></tr><tr><td>product2</td><td>Map(price -> 200, quantity -> 3)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         {
          "price": 100,
          "quantity": 2
         }
        ],
        [
         "product2",
         {
          "price": 200,
          "quantity": 3
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_info",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"long\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"product1\", {\"price\": 100, \"quantity\": 2}), \n",
    "        (\"product2\", {\"price\": 200, \"quantity\": 3})]\n",
    "columns = [\"product_id\", \"product_info\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6328ef-7353-4381-8afd-86f4391b5337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df.select('product_id', 'product_info.price', 'product_info.quantity').display()\n",
    "df.select('product_id', 'product_info.price', 'product_info.quantity').createOrReplaceTempView('flatview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4db66429-49d3-4457-ab6f-d3eaec1efff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>price</th><th>quantity</th></tr></thead><tbody><tr><td>product1</td><td>100</td><td>2</td></tr><tr><td>product2</td><td>200</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product1",
         100,
         2
        ],
        [
         "product2",
         200,
         3
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "product_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "price",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "quantity",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 189
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from flatview;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "423dc412-e75c-4f13-acf8-ee7bc6959424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**28. You are ingesting data from an external API in JSON format where the schema is inconsistent. How would you handle this situation to ensure a robust pipeline?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6ddd79-c5dd-4b69-a8e3-2a100929fc78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# scenario based\n",
    "df =  spark.read.format('json').option('mergeSchema', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87842c15-2f1b-4fe7-aee5-230627e8c6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**29. While reading data from Parquet, you need to optimize performance by partitioning the data based on a column. How would you implement this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0585eff6-7d7b-43f9-9886-2100608013da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**scenario based**\n",
    "df.write.format('parquet').mode('append').partitionBy('category').save('location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "134e2ed9-178d-4e15-8b8a-745e5f0024c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**30. You are working with a large dataset in Parquet format and need to ensure that the data is written in an optimized manner with proper compression. How would you accomplish this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8e79e1-d5b0-402f-b2da-e8c3c40c06fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**scenario based**\n",
    "df.write.format('parquet').option('compression', 'snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7d9ee2-00d2-4595-8dfb-ac2229cff548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**31. Your company uses a large-scale data pipeline that reads from Delta tables and processes data using complex aggregations. However, performance is becoming an issue due to the growing dataset size. How would you optimize the performance of the pipeline?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dab4793-660d-44f0-9e4c-d7a6f7b1e0c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "OPTIMIZE tabledelta ZORDER BY ('order_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a4450c-7e3b-498d-b6fe-9e608e43ba67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🔍Delta Table & Spark Aggregation Optimization Strategies\n",
    "\n",
    "✅ This scenario addresses a highly relevant issue in modern data engineering, as performance optimization for growing datasets is a key skill sought by companies. Since the pipeline reads from large Delta tables and performs complex aggregations, the optimization strategy should focus on both the Delta table structure and the Spark execution plan.\n",
    "\n",
    "The sources highlight the following key strategies for optimizing performance, especially concerning large Delta tables:\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ Delta Table Optimization using `OPTIMIZE` and `ZORDER BY`\n",
    "\n",
    "The most direct and impactful step to optimize performance when reading from large Delta tables is applying the `OPTIMIZE` command coupled with `ZORDER BY`.\n",
    "\n",
    "#### 🧱 A. `OPTIMIZE` Command (Partition Coalescing):\n",
    "\n",
    "The `OPTIMIZE` command addresses performance issues by **coalescing partitions**.\n",
    "\n",
    "* 🔄 It combines many smaller partitions into fewer, larger partitions.\n",
    "* 📏 This is based on the \"rule of thumb\" that it is always easier to read data from **bigger and fewer partitions** rather than smaller and numerous partitions.\n",
    "* 🤖 The system automatically determines the best partition size required.\n",
    "\n",
    "#### 🧭 B. `ZORDER BY` Command (Data Skipping):\n",
    "\n",
    "The `ZORDER BY` command should be used alongside `OPTIMIZE` to significantly boost query performance.\n",
    "\n",
    "* 🧮 It sorts the data *within* the partitions based on chosen columns (e.g., `order date`).\n",
    "* 🚀 When a query runs (e.g., `SELECT * FROM table WHERE ID <= 5`), Spark utilizes **column statistics** (available for the first 32 or 33 columns) that are generated for the Delta table.\n",
    "* ⛔ Because the data is sorted (Z-ordered), Spark knows the minimum and maximum values in each partition. If a partition's data range does not meet the query condition, Spark performs **Data Skipping**, avoiding the read entirely for that partition, which saves significant time.\n",
    "* 💡 *Implementation Example (using SQL syntax):*\n",
    "\n",
    "  ```sql\n",
    "  OPTIMIZE table Delta_table_name\n",
    "  ZORDER BY (order_date) \n",
    "  ```\n",
    "\n",
    "  (assuming `order_date` is the relevant column for ordering).\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Optimizing Complex Aggregations\n",
    "\n",
    "Since the pipeline includes complex aggregations, which often involve wide transformations like `GROUP BY` that trigger expensive data shuffling, the following strategies should be employed:\n",
    "\n",
    "#### ⚙️ A. Adaptive Query Execution (AQE):\n",
    "\n",
    "Adaptive Query Execution is a powerful optimization technique that optimizes query execution at runtime. It handles several complexities related to large-scale aggregations automatically:\n",
    "\n",
    "* 🎯 **Dynamically Optimizes Skewness:** AQE automatically mitigates data skewness (where some keys are heavily biased) by breaking overly large partitions into multiple smaller ones at runtime. This prevents \"Executor out of memory\" errors caused when skewed keys pull too much data to a single machine.\n",
    "* 🧹 **Dynamic Partition Pruning:** AQE can coalesce unnecessary small or empty partitions dynamically, ensuring efficient parallelism.\n",
    "* 🆗 AQE is generally auto-enabled in recent Spark versions (post 3.0/3.12).\n",
    "\n",
    "#### 🧩 B. Partition Management (Coalesce and Repartition):\n",
    "\n",
    "For manual control over partitions:\n",
    "\n",
    "* ➖ Use the `coalesce` function to **reduce** the number of partitions to a smaller, more manageable count, which does not require data shuffling.\n",
    "* 🔄 Use `repartition` to increase or decrease the number of partitions. Note that `repartition` requires data shuffling.\n",
    "\n",
    "#### 🧠 C. Caching Intermediate Results:\n",
    "\n",
    "If the results of a complex aggregation step (an intermediate data frame) are used multiple times later in the pipeline, store that result using `cache` or `persist`.\n",
    "\n",
    "* 💾 This avoids recomputing the data frame from scratch repeatedly, saving time.\n",
    "* 🧱 `cache` is equivalent to using `persist` with the storage level set to `MEMORY_AND_DISK`, meaning it attempts to store data in memory first and spills to disk if memory runs out.\n",
    "\n",
    "#### 🚚 D. Broadcast Joins (if complex aggregations involve joins):\n",
    "\n",
    "If the aggregation requires joining the large Delta table with a significantly smaller data frame (e.g., a lookup table), use a **broadcast join**.\n",
    "\n",
    "* 📡 A broadcast join copies the small data frame to all executor nodes, eliminating the expensive networking overhead and data shuffling typically required for standard joins.\n",
    "* 🧠 Spark automatically performs a broadcast join if the smaller data frame is below a set threshold, but it can be explicitly enforced using the `broadcast` keyword.\n",
    "\n",
    "#### 📦 E. Compression:\n",
    "\n",
    "Ensure the data is written with efficient compression, such as **Snappy** for Parquet format (often used internally by Delta Lake), to save storage cost and optimize query performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d619ce-b839-4d5e-bd2f-1621db0a6ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06d4f63-bede-4e83-a70c-05f7d0a6fe70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**43. You are processing sales data. Group by product categories and create a list of all product names in each category.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "246da98c-b379-45b8-88ed-fc267b842596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>product</th></tr></thead><tbody><tr><td>Electronics</td><td>Laptop</td></tr><tr><td>Electronics</td><td>Smartphone</td></tr><tr><td>Furniture</td><td>Chair</td></tr><tr><td>Furniture</td><td>Table</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Electronics",
         "Laptop"
        ],
        [
         "Electronics",
         "Smartphone"
        ],
        [
         "Furniture",
         "Chair"
        ],
        [
         "Furniture",
         "Table"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"Electronics\", \"Laptop\"), (\"Electronics\", \"Smartphone\"), (\"Furniture\", \"Chair\"), (\"Furniture\", \"Table\")]\n",
    "columns = [\"category\", \"product\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adebc6c0-8c9b-4a16-8d82-8753e326a824",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"products\":172},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758463969034}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>products</th></tr></thead><tbody><tr><td>Electronics</td><td>List(Laptop, Smartphone)</td></tr><tr><td>Furniture</td><td>List(Chair, Table)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Electronics",
         [
          "Laptop",
          "Smartphone"
         ]
        ],
        [
         "Furniture",
         [
          "Chair",
          "Table"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "products",
         "type": "{\"containsNull\":false,\"elementType\":\"string\",\"type\":\"array\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = df.groupBy('category').agg(collect_list('product').alias('products'))\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4fb590-28ca-46cc-9a54-96fa269c6fbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**44. You are analyzing orders. Group by customer IDs and list all unique product IDs each customer purchased.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bb0ae6-727e-4a43-bd25-c8779ac58105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>product_id</th></tr></thead><tbody><tr><td>101</td><td>P001</td></tr><tr><td>101</td><td>P002</td></tr><tr><td>102</td><td>P001</td></tr><tr><td>101</td><td>P001</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         "P001"
        ],
        [
         101,
         "P002"
        ],
        [
         102,
         "P001"
        ],
        [
         101,
         "P001"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(101, \"P001\"), (101, \"P002\"), (102, \"P001\"), (101, \"P001\")]\n",
    "columns = [\"customer_id\", \"product_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1726a0bc-265a-4bab-af0a-c187bf16a5c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>unique_products</th></tr></thead><tbody><tr><td>101</td><td>List(P001, P002)</td></tr><tr><td>102</td><td>List(P001)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         [
          "P001",
          "P002"
         ]
        ],
        [
         102,
         [
          "P001"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "unique_products",
         "type": "{\"containsNull\":false,\"elementType\":\"string\",\"type\":\"array\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupBy('customer_id').agg(collect_set('product_id').alias('unique_products')).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5df8a5-c803-401e-8852-11f343e26072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**45. For customer records, combine first and last names only if the email address exists.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9306ac-71a7-49b4-ad27-6b0d88045b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>first_name</th><th>last_name</th><th>email</th></tr></thead><tbody><tr><td>John</td><td>Doe</td><td>john.doe@example.com</td></tr><tr><td>Jane</td><td>Smith</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John",
         "Doe",
         "john.doe@example.com"
        ],
        [
         "Jane",
         "Smith",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "first_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"John\", \"Doe\", \"john.doe@example.com\"), (\"Jane\", \"Smith\", None)]\n",
    "columns = [\"first_name\", \"last_name\", \"email\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09045b6d-c115-4705-a217-77f6ade8b8be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>first_name</th><th>last_name</th><th>email</th><th>fullname</th></tr></thead><tbody><tr><td>John</td><td>Doe</td><td>john.doe@example.com</td><td>John-Doe</td></tr><tr><td>Jane</td><td>Smith</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John",
         "Doe",
         "john.doe@example.com",
         "John-Doe"
        ],
        [
         "Jane",
         "Smith",
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "first_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fullname",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    'fullname', \n",
    "    when(\n",
    "        col('email').isNotNull(), \n",
    "        concat_ws('-', col('first_name'), col('last_name'))).otherwise(None)).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4622de60-9578-41d8-9b2b-f793710c7722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**46. You have a DataFrame containing customer IDs and a list of their purchased product IDs. Calculate the number of products each customer has purchased.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d76050ba-2a7d-44f6-b209-5dca1cf0ba21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>product_ids</th></tr></thead><tbody><tr><td>1</td><td>List(prod1, prod2, prod3)</td></tr><tr><td>2</td><td>List(prod4)</td></tr><tr><td>3</td><td>List(prod5, prod6)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         [
          "prod1",
          "prod2",
          "prod3"
         ]
        ],
        [
         2,
         [
          "prod4"
         ]
        ],
        [
         3,
         [
          "prod5",
          "prod6"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_ids",
         "type": "{\"containsNull\":true,\"elementType\":\"string\",\"type\":\"array\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (1, [\"prod1\", \"prod2\", \"prod3\"]),\n",
    "    (2, [\"prod4\"]),\n",
    "    (3, [\"prod5\", \"prod6\"]),\n",
    "]\n",
    "myschema = \"customer_id INT ,product_ids array<STRING>\"\n",
    "\n",
    "df = spark.createDataFrame(data, myschema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d720bbc-8089-45c8-a3de-21373e940f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>product_ids</th><th>number_of_products</th></tr></thead><tbody><tr><td>1</td><td>List(prod1, prod2, prod3)</td><td>3</td></tr><tr><td>2</td><td>List(prod4)</td><td>1</td></tr><tr><td>3</td><td>List(prod5, prod6)</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         [
          "prod1",
          "prod2",
          "prod3"
         ],
         3
        ],
        [
         2,
         [
          "prod4"
         ],
         1
        ],
        [
         3,
         [
          "prod5",
          "prod6"
         ],
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_ids",
         "type": "{\"containsNull\":true,\"elementType\":\"string\",\"type\":\"array\"}"
        },
        {
         "metadata": "{}",
         "name": "number_of_products",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = df.withColumn('number_of_products', size(col('product_ids')))\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4292a69-4cf5-4363-996b-81ea5bd186de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**47. You have employee IDs of varying lengths. Ensure all IDs are 6 characters long by padding with leading zeroes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a441fca-326c-4747-a111-eb98478f69d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>123</td></tr><tr><td>4567</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1"
        ],
        [
         "123"
        ],
        [
         "4567"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"1\",),\n",
    "    (\"123\",),\n",
    "    (\"4567\",),\n",
    "]\n",
    "schema = [\"employee_id\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb6ff36-52ec-4d08-a4d2-c3d7d5c5f6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th></tr></thead><tbody><tr><td>000001</td></tr><tr><td>000123</td></tr><tr><td>004567</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "000001"
        ],
        [
         "000123"
        ],
        [
         "004567"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.withColumn('employee_id', lpad(col('employee_id'), 6, '0')).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03bf938e-f3b2-40f6-a7db-18ae224bb3c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**48. You need to validate phone numbers by checking if they start with \"91\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84ecc91-f5b6-4c6f-be67-24b890bf25ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>phone_number</th></tr></thead><tbody><tr><td>911234567890</td></tr><tr><td>811234567890</td></tr><tr><td>912345678901</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "911234567890"
        ],
        [
         "811234567890"
        ],
        [
         "912345678901"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "phone_number",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"911234567890\",),\n",
    "    (\"811234567890\",),\n",
    "    (\"912345678901\",),\n",
    "]\n",
    "schema = [\"phone_number\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028a947e-099c-45df-be32-5296158de874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>phone_number</th></tr></thead><tbody><tr><td>911234567890</td></tr><tr><td>912345678901</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "911234567890"
        ],
        [
         "912345678901"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "phone_number",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.filter(substring(col(\"phone_number\"), 1, 2) == '91').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4dc1f7-6a2e-4269-9e57-0042614b1302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**49. You have a dataset with courses taken by students. Calculate the average number of courses per student.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29bb3d2-6755-4a41-94f1-02314f0cd1fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>student_id</th><th>courses</th></tr></thead><tbody><tr><td>1</td><td>List(Math, Science)</td></tr><tr><td>2</td><td>List(History)</td></tr><tr><td>3</td><td>List(Art, PE, Biology)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         [
          "Math",
          "Science"
         ]
        ],
        [
         2,
         [
          "History"
         ]
        ],
        [
         3,
         [
          "Art",
          "PE",
          "Biology"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "student_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "courses",
         "type": "{\"containsNull\":true,\"elementType\":\"string\",\"type\":\"array\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (1, [\"Math\", \"Science\"]),\n",
    "    (2, [\"History\"]),\n",
    "    (3, [\"Art\", \"PE\", \"Biology\"]),\n",
    "]\n",
    "schema = [\"student_id\", \"courses\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e9cda2-467a-4619-8994-272bd8dd03b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>avg(course_size)</th></tr></thead><tbody><tr><td>2.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\": \"true\"}",
         "name": "avg(course_size)",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.withColumn('course_size', size('courses')).groupBy().agg(avg('course_size')).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d781de4-8fa4-43ae-8d64-93e484ff4e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**50. You have a dataset with primary and secondary contact numbers. Use the primary number if available; otherwise, use the secondary number.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c071b8e-ec3b-47a4-9577-8a177ed4b26e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>primary_contact</th><th>secondary_contact</th></tr></thead><tbody><tr><td>null</td><td>1234567890</td></tr><tr><td>9876543210</td><td>null</td></tr><tr><td>7894561230</td><td>4567891230</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         "1234567890"
        ],
        [
         "9876543210",
         null
        ],
        [
         "7894561230",
         "4567891230"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "primary_contact",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "secondary_contact",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (None, \"1234567890\"),\n",
    "    (\"9876543210\", None),\n",
    "    (\"7894561230\", \"4567891230\"),\n",
    "]\n",
    "schema = [\"primary_contact\", \"secondary_contact\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d1f1da-c0bb-4b0f-ad62-d76970b81dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>primary_contact</th><th>secondary_contact</th><th>contact</th></tr></thead><tbody><tr><td>null</td><td>1234567890</td><td>1234567890</td></tr><tr><td>9876543210</td><td>null</td><td>9876543210</td></tr><tr><td>7894561230</td><td>4567891230</td><td>7894561230</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         "1234567890",
         "1234567890"
        ],
        [
         "9876543210",
         null,
         "9876543210"
        ],
        [
         "7894561230",
         "4567891230",
         "7894561230"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "primary_contact",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "secondary_contact",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.withColumn('contact', coalesce(col(\"primary_contact\"), col('secondary_contact'))).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0949793-981d-41f4-8370-e4bf900c51c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**51. You are categorizing product codes based on their lengths. If the length is 5, label it as \"Standard\"; otherwise, label it as \"Custom\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b63d5a-1ffb-42fc-902e-3198e2336679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_code</th></tr></thead><tbody><tr><td>prod1</td></tr><tr><td>prd234</td></tr><tr><td>pr9876</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "prod1"
        ],
        [
         "prd234"
        ],
        [
         "pr9876"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_code",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"prod1\",),\n",
    "    (\"prd234\",),\n",
    "    (\"pr9876\",),\n",
    "]\n",
    "schema = [\"product_code\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f301af4b-bb1c-48dd-a575-8eb6ec3dff8e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758909584565}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_code</th><th>code_flag</th></tr></thead><tbody><tr><td>prod1</td><td>Standard</td></tr><tr><td>prd234</td><td>Custom</td></tr><tr><td>pr9876</td><td>Custom</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "prod1",
         "Standard"
        ],
        [
         "prd234",
         "Custom"
        ],
        [
         "pr9876",
         "Custom"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "code_flag",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.withColumn('code_flag', when(length(col(\"product_code\"))==5, \"Standard\").otherwise(\"Custom\")).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7842992746430564,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark-interview-questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
